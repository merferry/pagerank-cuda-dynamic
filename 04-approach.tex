When dealing with a batch update comprising edge deletions $(u, v) \in \Delta^{t-}$ and insertions $(u, v) \in \Delta^{t+}$, if the total update size $|\Delta^{t-} \cup \Delta^{t+}|$ is relatively small compared to the total edge count $|E|$, only a small fraction of vertices are expected to undergo rank changes. To handle this, our earlier work proposed \textbf{Dynamic Frontier (DF)} and \textbf{Dynamic Frontier with Pruning (DF-P)} approaches, which employ an incremental process to identify affected vertices and update their ranks. Our research showed that DF/DF-P PageRank outperformed Static and Dynamic Traversal (DT) PageRank by $5.2\times$/$15.2\times$ and $1.3\times$/$3.5\times$\ignore{respectively} on real-world dynamic graphs, and by $7.2\times$/$9.6\times$ and $4.0\times$/$5.6\times$ on large graphs with random batch updates \cite{sahu2024df}.

Unfortunately, multicore CPUs have limited memory bandwidth and parallelism, making them unsuitable for graph algorithms like PageRank, which have a low computation-to-communication ratio. In contrast, GPUs offer high-bandwidth memory, connected in close proximity to thousands of lightweight cores with user-managed caches. Moreover, GPU hardware is designed to switch running threads at no cost to support memory access latency hiding. When appropriately designed, graph algorithms can significantly outperform CPU-based implementations on GPUs. This makes a GPU implementation of DF and DF-P PageRank attractive. In this section, we describe the design of DF and DF-P PageRank for GPUs.



\subsection{Design of Dynamic Frontier (DF) and Dynamic Frontier with Pruning (DF-P) PageRank for GPUs}
\label{sec:frontier}

\paragraph{Copying data to the device:}

A GPU has its own memory that supports a high volume of data transfers. Accordingly, we\ignore{first} copy the Compressed Sparse Row (CSR) representation of the current graph $G^t$, its transpose $G^{t'}$, the previous ranks of each vertex $R^{t-1}$, and the batch update consisting of edge deletions $\Delta^{t-}$ (both source and target vertex IDs in separate arrays) and edge insertions $\Delta^{t+}$ (source vertex IDs only). The CSR of $G^{t'}$ is used for computing PageRank scores, while that of $G^t$ is used for the initial and incremental marking of affected vertices. Note that we only need the source vertex IDs of edge insertions $(u, v) \in \Delta^{t+}$ as we only need to mark the outgoing neighbors of $u$ as affected. On the other hand, for edge deletions $(u, v) \in \Delta^{t-}$, we need both the source and target vertex IDs as we need to mark both the outgoing neighbors of $u$ and the target vertices of the edge deletions, i.e., $v$, as affected.

\paragraph{Partitioning vertices into low-degree and high-degree sets:}

Unlike threads on a multicore CPU, threads on a GPU are organized in hierarchy of warps, thread blocks, and grids. A warp consists of a group of threads that execute instructions is a fully synchronous manner, i.e., in lockstep. NVIDIA GPUs typically have a warp size of 32 threads. Further, a thread block consists of a group of threads which execute on the same Streaming Multiprocessor (SM) --- each warp in a thread block is executed in lockstep, and the SM schedules another warp in the thread block for execution, when one or more threads in the current warp are stalled (for example, due to a memory request). Since all threads in a thread block are executing on the same SM, they can readily communicate among themselves through a user-managed cache (also known as shared memory) that is private to each SM. Finally, a grid consists of a group of thread blocks, where each thread ...

We use a pair of kernels (functions executed on the GPU), i.e., a thread-per-vertex kernel and a block-per-vertex kernel, each for rank computation and incremental marking of affected vertices. A thread-per-vertex kernel is used to process low degree vertices, while a block-per-vertex kernel is used to process high-degree vertices. Using two different kernels allows us to improve processing performance for high-degree vertices, while minimizing resource wastage for low-degree vertices. For this, we partition the vertex IDs for the two kernels by their in-degree for the rank computation phase (the work to be performed by each thread is proportional to the in-degree of each vertex during the rank computation phase), and by their out-degree for the incremental marking of affected vertices (the work to be performed by each thread is proportional to the out-degree of each vertex during the incremental marking). This also helps minimize thread divergence with the thread-per-vertex kernel. The psuedocode for partitioning the vertices (by in- or out-degree of vertices) is given in Algorithm \ref{alg:partition}, with its explanation given in Section \ref{sec:partition}.

\paragraph{Marking the initial set of affected vertices:}

Upon each edge deletion $(u, v) \in \Delta^{t-}$ and edge insertion $(u, v) \in \Delta^{t+}$ in the batch update, with the DF and DF-P approaches, we need to mark the outgoing neighbors of vertex $u$ in both the prior snapshot $G^{t-1}$ and the current snapshot $G^t$ as affected. This is equivalent to marking the outgoing neighbors of $u$ for each edge deletion and insertion $(u, v) \in \Delta^{t-} \cup\Delta^{t+}$ as affected, and marking the target vertices of $v$ for each edge deletion $(u, v) \in \Delta^{t-}$ as affected. In order to achieve this, we use one kernel to the target vertices of $v$ for each edge deletion $(u, v) \in \Delta^{t-}$ as affected, and use a temporary array to indicate that the outgoing neighbors of $u$ for each edge deletion and insertion $(u, v) \in \Delta^{t-} \cup\Delta^{t+}$ need to be marked as affected later. A thread-per-vertex kernel and block-per-vertex kernel and then used to actually mark the outgoing neighbors of such vertices as affected. The vertex IDs between the two kernels are partitioned by out-degree (the work to be performed by each thread is proportional to the out-degree of each vertex during the marking), the thread divergence is expected to be minimal. Details of why this is done is given later.

\paragraph{Rank computation of each vertex, and incremental contraction of the set of affected vertices:}

We observe that, unlike on multicore CPUs, a synchronous implementation of the PageRank algorithm, which uses two separate rank vectors offers better performance than an asynchronous approach (which performs well on multicore CPUs). Accordingly, we use a synchronous implementation of the PageRank algorithm.

For the rank computation phase in each iteration, we use two different kernels --- a thread-per-vertex kernel for processing vertices with low in-degree, and a block-per-vertex kernel for processing high in-degree vertices. The thread-per-vertex kernel schedules one thread per vertex, and updated the rank of each vertex independently (in a sequential manner). On the other hand, the block-per-vertex kernels schedules a thread block per vertex, where each thread in a thread block computes the rank contribution of in-neighbors to the vertex in a strided fashion. These partial rank contributions are the written to shared memory, and a block reduce (sum) is performed to obtained the net rank contribution. The first thread in the thread block then updates rank of the given vertex. The vertex IDs between the two kernels are partitioned by in-degree (the work to be performed by each thread is proportional to the in-degree of each vertex during the rank computation phase), and hence, the thread divergence is expected to be minimal.

With the DF and DF-P approaches, if the relative change in the rank of a vertex $u$ is greater than the frontier tolerance $\tau_f$, we need to incrementally mark the outgoing neighbors $v \in G^t.out(u)$ of $u$ as affected \cite{sahu2024df}. However, performing this incremental marking during the rank computation phase may introduce significant thread divergence. This is because rank computation is performed using a thread-per-vertex and a block-per-vertex kernel, with the vertex IDs to be processed by each of the kernels being partitioned between the two by the in-degrees of the vertices, while the incremental marking of affected vertices need to processed by two similar kernels, with the vertex IDs being partitioned by their out-degree (the work to be performed by each thread is proportional to the out-degree of each vertex during the incremental marking) and not their in-degree. Since SMs execute warps in lockstep, this would mean that many threads would simply be waiting for the longest thread to complete, resulting is poor GPU utilization, and thus poor performance. Thus, for incremental marking of affected vertices, we simply use a temporary array to indicate that the neighbors of an vertex need to be incrementally marked as affected during rank computation. This can later be used with a pair of kernels to actually mark the outgoing neighbors of such vertices as affected.

With the DF-P approach, if the relative change in the rank of a vertex $u$ is within than the prune tolerance $\tau_p$, we need to mark the $u$ as not affected \cite{sahu2024df}. This is done directly unflagging it in the set of affected vertices, as it is $O(1)$ work, and does not introduce any significant thread divergence.

In the context of DF-P PageRank, vertices have the potential to be pruned, indicating that they are marked as not affected. Given the inclusion of a self-loop for each vertex in the graph (as discussed in Sections \ref{sec:dataset} and \ref{sec:batch-generation}), we utilize a closed-loop formula for computing the rank of each vertex (Equation \ref{eq:pr-prune}). This formula is designed to accommodate the presence of the self-loop, thereby mitigating the necessity for recursive rank calculations arising from it \cite{sahu2024df}.

\begin{flalign}
\label{eq:pr-prune}
  R[v] & = \frac{1}{1 - \alpha / |G.out(v)|} \left(\alpha K + \frac{1 - \alpha}{|V|}\right) && \\
    \text{where, } K & = \left(\sum_{u \in G.in(v)} \frac{R[u]}{|G.out(u)|}\right) - \frac{R[v]}{|G.out(v)|}
\end{flalign}

\paragraph{Incremental marking of affected vertices:}

With the DF and DF-P approaches, if the relative change in the rank of a vertex $u$ is greater than the frontier tolerance $\tau_f$, we need to incrementally mark the outgoing neighbors $v \in G^t.out(u)$ of $u$ as affected. However, performing this incremental marking during the rank computation phase may introduce significant thread divergence. This is because rank computation is performed using a thread-per-vertex and a block-per-vertex kernel, with the vertex IDs to be processed by each of the kernels being partitioned between the two by the in-degrees of the vertices, while the incremental marking of affected vertices need to processed by two similar kernels, with the vertex IDs being partitioned by their out-degree and not their in-degree. Since SMs execute warps in lockstep, this would mean that many threads would simply be waiting for the longest thread to complete, resulting is poor GPU utilization, and thus poor performance. Thus, for incremental marking of affected vertices, we simply use a temporary array to indicate that the neighbors of an vertex need to be incrementally marked as affected during rank computation, and later use a pair of thread-per-vertex and block-per-vertex kernels to actually mark the outgoing neighbors of such vertices as affected. Since the vertex IDs between the two kernels are partitioned by out-degree (the work to be performed by each thread is proportional to the out-degree of each vertex during the incremental marking), the thread divergence is expected to be minimal.\ignore{After the incremental marking is complete, the temporary array can be cleared for the next iteration.}

\paragraph{Convergence detection:}

To determine if the ranks of vertices have converged, we calculate the $L_\infty$-norm of the difference between the current and previous ranks. If this value is below the specified iteration tolerance $\tau$, the algorithm terminates, indicating convergence. If not, we swap the current and previous ranks and proceed to the next iteration. The calculation of the $L_\infty$-norm involves two kernels. The first kernel computes the $L_\infty$-norm of the rank differences for each thread block in a grid and stores the results in a temporary buffer. The second kernel computes the net $L_\infty$-norm of the results in the buffer, which is then transferred to the CPU.

WHAT PARAMETER VALUES DO WE USE [CITE].


\subsubsection{A simple example}

Figure \ref{fig:about-frontier} provides an illustration of DF and DF-P PageRank. Initially, as demonstrated in Figures \ref{fig:about-frontier-df1} and \ref{fig:about-frontier-dfp1}, the graph consists of $16$ vertices and $23$ edges. Subsequent to this, Figures \ref{fig:about-frontier-df2} and \ref{fig:about-frontier-dfp2} display a batch update executed on the original graph, entailing an edge insertion from vertex $4$ to $12$ and an edge deletion from vertex $2$ to $1$. Post-batch update, we proceed with the initial phase of DF/DF-P PageRank, wherein we identify and mark the outgoing neighbors of vertices $2$ and $4$ as affected, specifically vertices $1$, $8$, $12$, and $14$. These affected vertices are distinguished with a yellow fill. It is noteworthy that vertices $2$ and $4$ remain unmarked as affected. This is due to the fact that changes in a vertex's out-degree do not influence its PageRank score (refer to Equation \ref{eq:pr}). Subsequently, we commence the first iteration of the PageRank algorithm.

In the first iteration (depicted in Figures \ref{fig:about-frontier-df3} and \ref{fig:about-frontier-dfp3}), the ranks of affected vertices undergo updating. Now, say the relative change in rank of vertices $1$, $8$, $12$, and $14$ surpasses the frontier tolerance $\tau_f$ --- these vertices are highlighted with a red border in the figures. Consequently, in both DF and DF-P PageRank, we proceed to incrementally mark the outgoing neighbors of vertices $1$, $8$, $12$, and $14$ as affected. Specifically, vertices $3$, $5$, $9$, $10$, $14$, and $15$ are identified as such.

\input{src/fig-about-frontier}

In the second iteration, as illustrated in Figures \ref{fig:about-frontier-df4} and \ref{fig:about-frontier-dfp4}, another round of updates is applied to the ranks of the impacted vertices. Notably, the ranks of vertices $3$, $5$, $9$, $14$, and $15$ exhibit a relative change exceeding the designated frontier tolerance $\tau_f$. Consequently, employing DF/DF-P PageRank, we identify the outgoing neighbors of these vertices, namely vertices $4$, $6$, $10$, $15$, and $16$, as affected. Conversely, the relative change in rank of vertices $1$, $8$, and $12$ remains below the prune tolerance threshold $\tau_p$. Consequently, utilizing DF-P PageRank, these vertices are no longer classified as affected, indicating a probable convergence of their ranks. This action effectively contracts the frontier of affected vertices. However, if a vertex's rank has not yet converged, it might be re-designated as affected by one of its in-neighbors. Subsequently, in the ensuing iteration, the ranks of affected vertices undergo further updates. Should the change in rank for each vertex fall within the defined iteration tolerance $\tau$\ignore{(we use $L\infty$-norm for convergence detection)}, it signifies convergence of the ranks, and the algorithm halts.

\paragraph{Contrasting with Dynamic Traversal (DT) PageRank:}

We now compare DF and DF-P PageRank with DT PageRank (see Figures \ref{fig:about-frontier-dt1}-\ref{fig:about-frontier-dt4}). In Figure \ref{fig:about-frontier-dt2}, the identical batch update applied to the original graph is depicted, akin to Figures \ref{fig:about-frontier-df2} and \ref{fig:about-frontier-dfp2}. In response to this update, DT PageRank designates all vertices reachable from $2$ and $4$ as affected, i.e., all vertices except $7$, $11$, and $13$. Subsequently, the ranks of this subset of affected vertices undergo updates in each iteration\ignore{(while the ranks of unaffected vertices remain unchanged)}, continuing until convergence is achieved.

%% Describe how you determing the correct partitioning approach




\subsection{Determining suitable Partitioning approach}
\label{sec:parition-determine}

In order to optimize the performance of DF and DF-P PageRank for the GPU, we attempt three different techniques work distribution between the thread-per-vertex and block-per-vertex kernels for updating ranks of vertices in the graph and incrementally marking affected vertices.

With the first technique, which we refer to as \textit{Don't Partition}, we do not partition the graph, and instead selectively execute the thread/block-per-vertex kernels on each vertex, depending on the in/out-degree of the vertex --- for both the rank computation phase and the incremental marking of affected vertices. With the second technique, which we refer to as \textit{Partition $G'$} ($G'$ stands for transpose of $G$, the current graph), we partition the graph into low in-degree and high in-degree vertices, and run the kernels on respective partitions for updating ranks --- the incremental marking of affected vertices is still done selectively. With the third technique, which we refer to as \textit{Partition $G$, $G'$}, we partition the graph by both in-degrees and out-degrees, and run the kernels of respective partitions (i.e., thread-per-vertex kernel on low degree vertices, and block-per-vertex kernel on high degree vertices) for both rank computation and incremental marking of affected vertices.

Results, as shown in Figure \ref{fig:adjust-partition}, indicate that the \textit{Partition $G$, $G'$} approach performs the best. Note that partitioning the vertex IDs by out-degree, i.e., \textit{Partition $G$}, is useful for incremental marking of affected vertices, but comes with added runtime cost --- hence the small improvement in performance when moving from \textit{Partition $G'$} to \textit{Partition $G$, $G'$}.

\input{src/fig-adjust-partition}
\input{src/alg-frontier}
\input{src/alg-update}
\input{src/alg-partition}
\input{src/alg-affected}




\subsection{Our DF* PageRank implementation}

Algorithm \ref{alg:frontier} presents the psuedocode of GPU-based DF and DF-P PageRank, which aims to efficiently compute PageRank on large-scale graphs with dynamic updates. The algorithm takes several inputs, including the current graph snapshot $G^t$ and its transpose $G^{t'}$, edge deletions $\Delta^{t-}$ and insertions $\Delta^{t+}$, and the previous rank vector $R^{t-1}$. It returns the updated rank vector $R$.

The algorithm starts by initializing the rank vectors $R$ and $R_{new}$ with the previous rank vector $R^{t-1}$ (line \ref{alg:frontier--initialize}). Forst we partition the vertex IDs based on their out- and in-degree, aiming for efficient computation on the GPU (lines \ref{alg:frontier--partition-begin}-\ref{alg:frontier--partition-end}). Then, we mark the initial set of affected vertices using the provided edge deletions and insertions, and expand the affected set to include relevant neighbor vertices (lines \ref{alg:frontier--mark-begin}-\ref{alg:frontier--mark-end}). Subsequently, we start with PageRank iterations (lines \ref{alg:frontier--compute-begin}-\ref{alg:frontier--compute-end}), continuing until either the maximum number of iterations $MAX_ITERATIONS$ is reached or the change in ranks falls below the specified tolerance $\tau$ (line \ref{alg:frontier--converged}). Within each iteration, we update the rank vector $R_{new}$ based on the affected vertices (line \ref{alg:frontier--update}), while marking the vertices whose neighbors must be incrementally marked as affected (with relative change in rank greater than the frontier tolerance $\tau_f$) or contracting the set of affected vertices if the change in rank of a vertex is small (with relative change in rank below the prune tolerance $\tau_p$, only with DF-P PageRank). The $L_\infty$-norm difference between the current $R_{new}$ and previous ranks $R$ is then computed to check for convergence, and the rank vectors are swapped for the next iteration (line \ref{alg:frontier--error}). In line \ref{alg:frontier--converged}, we perform a convergence check. If convergence has not yet been achieved, we incrementally expand the set of affected vertices (line \ref{alg:frontier--remark}) from the vertices identified during rank computation (line \ref{alg:frontier--update}). Finally, we return the updated rank vector $R$ (line \ref{alg:frontier--return}), providing the computed PageRank scores for each vertex in the graph after dynamic updates.

In a push-based approach for PageRank computation, each thread calculates and sums the outgoing PageRank contribution of its vertex to its neighbors, necessitating atomic updates. In contrast, with a pull-based approach, each vertex's rank is updated through a single write by a thread \cite{verstraaten2015quantifying}. We find this to be more efficient and employ it for all implementations \cite{sahu2024df}. Furthermore, we employ an synchronous implementation of DF and DF-P PageRank, using two separate rank vectors (one is the input rank vector, and other is the output), which we observe to perform better than an asynchronous implementation. This is in contrast to our multicore CPU implementation of DF and DF-P PageRank, where we observe that an asynchronous implementation offers better performance \cite{sahu2024df, sahu2024incrementally}. We also utilize synchronous implementations for Static, Naive-dynamic (ND), and Dynamic Traversal (DT) PageRank.


\subsubsection{Updating ranks of vertices in parallel}

Algorithm \ref{alg:update} provides a psuedocode for updating the ranks of vertices in parallel. Here, the function \texttt{updateRanks()} takes as input the set of affected vertices $\delta_V$, affected neighbor vertices $\delta_N$, the previous and current rank vectors $R$ and $R_{new}$, respectively, the current input graph $G^t$, partitioned vertex IDs $P'$ (low in-degree vertices come first), and the number of vertices with low in-degree $N'_P$. It also requires parameters such as the damping factor $\alpha$, frontier tolerance $\tau_f$, and prune tolerance $\tau_p$.

The algorithm operates in two phases: a thread-per-vertex kernel (for low degree vertices) and a block-per-vertex kernel (for high degree vertices). In the \textit{thread-per-vertex kernel} (lines \ref{alg:update--thread-begin}-\ref{alg:update--thread-end}), we use each thread to process each low degree vertex in parallel, iterating over the partitioned vertex IDs ($P'$). For each vertex $v$, if it's not marked as affected, we skip it (line \ref{alg:update--affected}). Otherwise, we compute the new rank $r$ based on the incoming edges and ranks of neighboring vertices (lines \ref{alg:update--rank-begin}-\ref{alg:update--rank-end}). Depending on whether DF or DF-P PageRank is selected, we compute ranks using either Equation \ref{eq:pr} or \ref{eq:pr-prune}, respectively. Next, we compute the change in rank $\Delta r$ of the current vertex $v$ from its previous iteration (line \ref{alg:update--change}). If the relative change in rank of $v$, i.e., $\Delta r / \max(r, R[v])$, is within the prune tolerance $\tau_p$, we perform pruning by marking $v$ and no longer affected. However, if the relative change in rank of $v$ is above the frontier tolerance $\tau_f$, we mark the vertices whose neighbors must be incrementally marked as affected (the incremental marking of affected vertices is performed at a later point of time, using the \texttt{expandAffected()} function (Algorithm \ref{alg:affected}). Finally, we update the rank of vertex $v$ in the $R$ vector. In the \textit{block-per-vertex kernel} (lines \ref{alg:update--block-begin}-\ref{alg:update--block-end}), we use each thread block to process each high degree vertex in parallel, utilizing block-level parallelism. It involves similar operations as the thread-per-vertex kernel, but uses block-reduce operations and shared memory.


\subsubsection{Parallel vertex partitioning by degree}

Algorithm \ref{alg:partition} outlines the psuedocode for parallel vertex partitioning by degree. It aims to split the vertices of the input graph $G(V, E)$ into two groups based on their degree: low-degree vertices and high-degree vertices. The algorithm provides partitioned vertex IDs $P$ with low-degree vertices being listed first, along with the number of vertices with low degree $N_P$, as its output.

In the function \texttt{partition()}, we start by initializing an empty buffer $B_k$ and the set of partitioned vertex IDs $P$ (line \ref{alg:partition--initialize}). We then proceed to populate $B_k$ with boolean values indicating whether each vertex has a degree less than or equal to a specified threshold $D_P$ (lines \ref{alg:partition--less-begin}-\ref{alg:partition--less-end}). Afterward, we perform an exclusive prefix sum operation on $B_k$ to determine the number of low-degree vertices $N_P$ (lines \ref{alg:partition--lscan-begin}-\ref{alg:partition--lscan-end}). In the subsequent loop, we assign low-degree vertices to the appropriate positions in the partitioned vertex IDs array $P$ (lines \ref{alg:partition--lpopulate-begin}-\ref{alg:partition--lpopulate-end}). We then repeat a similar process for high-degree vertices. We populate $B_k$ with boolean values indicating whether each vertex has a degree greater than $D_P$ (lines \ref{alg:partition--greater-begin}-\ref{alg:partition--greater-end}), perform another exclusive prefix sum operation on $B_k$ (line \ref{alg:partition--gscan}), and assign high-degree vertices to the appropriate positions in $P$ (lines \ref{alg:partition--gpopulate-begin}-\ref{alg:partition--gpopulate-end}). Finally, we return the partitioned vertex IDs $P$ along with the number of low-degree vertices $N_P$ (line \ref{alg:partition--return}).


\subsubsection{Parallel marking of affected vertices}

Algorithm \ref{alg:affected} presents the psuedocode for parallel marking of affected vertices, consisting of two functions: \texttt{initialAffected()} and \texttt{expandAffected()}.

The \texttt{initialAffected()} function performs an initial marking step of DF and DF-P PageRank. It takes as input the current graph snapshot $G^t$ and the sets of edge deletions $\Delta^{t-}$ and insertions $\Delta^{t+}$. Here, we first initialize two arrays, $\delta_V$ and $\delta_N$, which represent whether each vertex and its neighbors are affected, respectively (line \ref{alg:affected--iinitialize}). Next, for each edge deletion in $\Delta^{t-}$, we mark both the source and target vertices as affected (lines \ref{alg:affected--idel-begin}-\ref{alg:affected--idel-end}). Similarly, for each edge insertion in $\Delta^{t+}$, we mark the source vertex as affected (lines \ref{alg:affected--iins-begin}-\ref{alg:affected--iins-end}). Finally, we return $\delta_V$ and $\delta_N$ (line \ref{alg:affected--ireturn}).

The \texttt{expandAffected()} function propagates the affected status to neighboring vertices. It takes as input flags indicating whether each vertex is affected $\delta_V$ or its neighbors are affected $\delta_N$, the current graph snapshot $G^t$, partitioned vertex IDs $P$ with low degree vertices placed first, and the number of vertices with low degree $N_P$. This algorithm also operates in two phases: a thread-per-vertex kernel (for low degree vertices) and a block-per-vertex kernel (for high degree vertices). In the \textit{thread-per-vertex kernel} (lines \ref{alg:affected--ethread-begin}-\ref{alg:affected--ethread-end}), we use each thread to process each low degree vertex in parallel by iterating through the partitioned vertex IDs array $P$. For each vertex $u$ marked as affected in $\delta_N$, we invoke the \texttt{markNeighbors()} function to mark its neighbors as affected in $\delta_V$ (line \ref{alg:affected--etmark}). In the \textit{block-per-vertex kernel} (lines \ref{alg:affected--eblock-begin}-\ref{alg:affected--eblock-end}), we use each thread block to process each vertex in parallel, utilizing block-level parallelism. It involves similar operations as the thread-per-vertex kernel.
