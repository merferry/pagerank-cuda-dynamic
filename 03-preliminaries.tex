\subsection{PageRank algorithm}
\label{sec:pagerank}

The PageRank, represented as $R[v]$, of a vertex $v \in V$ in the graph $G(V, E)$, measures its importance based on incoming links. Equation \ref{eq:pr} describes the PageRank computation for vertex $v$ in graph $G$, where $V$ denotes the set of vertices, $E$ represents the set of edges, $G.in(v)$ and $G.out(v)$ denote incoming and outgoing neighbors of $v$, respectively, and $\alpha$ is the damping factor. Initially, each vertex has a PageRank of $1/|V|$. The \textit{power-iteration} method iteratively updates these values until they converge within a specified iteration tolerance $\tau$. This is often measured using the $L_1$-norm \cite{ohsaka2015efficient}, though $L_2$ and $L_\infty$-norm are also occasionally used.

Core to the PageRank algorithm is the random surfer model. It conceives a surfer navigating the web by following links on each page. The damping factor $\alpha$, defaulting to $0.85$, indicates the likelihood of the surfer continuing along a link rather than jumping randomly. PageRank for each page reflects the long-term probability of the surfer visiting that page, originating from a random page and following links. PageRank values essentially constitute the eigenvector of a transition matrix, encoding probabilities of page transitions in a Markov Chain.

Dead ends, also referred to as dangling vertices, present a significant challenge in PageRank computation due to their lack of out-links, which compels the surfer to transition to a random web page. As a consequence, dead ends evenly distribute their rank across all vertices in the graph, necessitating computation in each iteration, thereby incurring overhead. We tackle this problem by introducing self-loops to all vertices in the graph \cite{kolda2009generalized, rank-andersen07, rank-langville06}, a strategy observed to be particularly effective in streaming environments and spam-link applications \cite{kolda2009generalized}.

\begin{equation}
\label{eq:pr}
    R[v] = \alpha \times \sum_{u \in G.in(v)} \frac{R[u]}{|G.out(u)|} + \frac{1 - \alpha}{|V|}
\end{equation}




\subsection{Fundamentals of a GPU}

The fundamental building block of NVIDIA GPUs is the Streaming Multiprocessor (SM). Each SM consists of multiple CUDA cores, which are responsible for executing parallel threads. SMs also include shared memory, registers, and special function units. The number of SMs varies across different GPU models, and each SM operates independently, executing multiple threads in parallel \cite{cuda-sanders10, gpu-nickolls10}.

The memory hierarchy of NVIDIA GPUs includes global memory, shared memory, and local memory. Global memory is the largest and slowest memory type, providing high-capacity storage for data. Shared memory is a fast, low-latency memory that is shared among threads within an SM, facilitating efficient data sharing and communication. It is used to store data that is frequently accessed by multiple threads, which can improve performance by reducing the number of memory accesses. Local memory is per-thread private memory used to store variables that do not fit in registers \cite{cuda-sanders10, gpu-nickolls10}.

\ignore{\paragraph{CUDA Cores}}

\ignore{CUDA cores are the primary execution units within an SM. These cores are responsible for executing individual threads in parallel. Each CUDA core is capable of performing arithmetic and logic operations, as well as handling memory access and control flow. The number of CUDA cores per SM varies across different GPU architectures, with newer generations featuring an increased number of cores \cite{cuda-sanders10, gpu-nickolls10}.}

\ignore{\paragraph{Behavioral fundamentals}}

Threads on a GPU are structured differently from those on a multicore CPU, organized into a hierarchy of warps, thread blocks, and grids. A warp comprises threads executing instructions in synchronous manner, i.e., in lockstep. NVIDIA GPUs typically have a warp size of 32 threads. Thread blocks, on the other hand, are collections of threads executing on the same SM. Within a thread block, warps execute in lockstep, with the SM scheduling alternate warps if any threads stall, such as during memory requests. All threads within a block can communicate through a user-managed cache, known as shared memory, which is private to each SM. Lastly, a grid encompasses multiple thread blocks, each functioning independently. Grids offer a higher-level structure for managing parallelism, optimizing GPU resource utilization. Thread blocks within a grid communicate solely through global memory\ignore{, which, although slower than shared memory, facilitates data exchange between different blocks}.





\subsection{Dynamic Graphs}
\label{sec:about-dynamic}

A dynamic graph can be construed as a succession of graphs, where $G^t(V^t, E^t)$ denotes the graph at time step $t$. The transitions between successive time steps $t-1$ and $t$, from $G^{t-1}(V^{t-1}, E^{t-1})$ to $G^t(V^t, E^t)$, can be denoted as a batch update $\Delta^t$ at time step $t$. This update encompasses a collection of edge deletions $\Delta^{t-}$, defined as $\{(u, v)\ |\ u, v \in V\} = E^{t-1} \setminus E^t$, and a set of edge insertions $\Delta^{t+}$, defined as $\{(u, v)\ |\ u, v \in V\} = E^t \setminus E^{t-1}$.


\ignore{\paragraph{Interleaving graph updates with computation:}}

\ignore{We consider graph updated to be batched, with changes to the graph structure and the algorithm execution being interleaved, permitting only one writer on the graph structure concurrently at any given time. If parallel updating of the graph structure is necessary during computation, a graph snapshot is needed to be obtained, upon which the computation may be performed (isolating it from the graphs updates). See for instance, the Aspen graph processing framework which reduces snapshot acquisition costs \cite{graph-dhulipala19}.}




\subsection{Existing approaches for updating PageRank on Dynamic Graphs}

\subsubsection{Naive-dynamic (ND) approach}
\label{sec:about-naive}

This approach simply updates vertex ranks in dynamic networks by initializing them with ranks from the previous graph snapshot and executing the PageRank algorithm on all vertices. Rankings obtained with this method are at least as accurate as those from the static algorithm.\ignore{Zhang et al. \cite{rank-zhang17} have explored the \textit{Naive-dynamic} approach in the hybrid CPU-GPU setting.}


\subsubsection{Dynamic Traversal (DT) approach}
\label{sec:about-traversal}

The Dynamic Traversal (DT) approach was initially proposed by Desikan et al. \cite{rank-desikan05}. It entails skipping the processing of vertices unaffected by the given batch update. For each edge deletion or insertion $(u, v)$ in the batch update, all the vertices reachable from vertex $u$ in either graph $G^{t-1}$ or $G^t$ are flagged as affected, typically using DFS or BFS.\ignore{Giri et al. \cite{rank-giri20} have explored the \textit{Dynamic Traversal} approach in the hybrid CPU-GPU setting. On the other hand, Banerjee et al. \cite{rank-sahu22} have explored this approach in the CPU and GPU settings separately where they compute the ranks of vertices in topological order of strongly connected components (SCCs) to minimize unnecessary computation. They borrow this ordered processing of SCCs from the original static algorithm proposed by Garg et al. \cite{rank-garg16}.}
