\input{src/fig-temporal-sx-mathoverflow}
\input{src/fig-temporal-sx-askubuntu}
\input{src/fig-temporal-sx-superuser}
\input{src/fig-temporal-wiki-talk-temporal}
\input{src/fig-temporal-sx-stackoverflow}

\clearpage

\section{Appendix}

\subsection{Derivation of Closed loop formula for Rank calculation towards Dynamic Frontier with Pruning (DF-P) PageRank}
\label{sec:pr-prune-derivation}

We proceed to derive the closed-loop formula for rank calculation with DF-P PageRank. As outlined in Sections \ref{sec:dataset} and \ref{sec:batch-generation}, self-loops are added to each vertex to circumvent the need for a global teleport rank computation in every iteration, thus reducing overhead. In DF-P PageRank, our aim is to skip the computation of ranks for vertices likely to have already converged. However, the existence of self-loops causes a delay in vertex rank convergence due to the immediate recursive nature they introduce. For instance, if the ranks of all in-neighbors of a vertex have already converged, the presence of self-loops inhibits the convergence of the vertex's rank in a single iteration. Nevertheless, we can mitigate this convergence issue by employing a closed-loop formula for the rank calculation of each vertex.

To achieve this, let us denote $r_0$ as the initial rank of a vertex $v$, $\alpha$ as the damping factor, $c = \sum_{u \in G.in(v)\ |\ u \neq v} \frac{R[u]}{|G.out(u)|}$ as the total rank contribution from its in-neighbors (excluding itself), $d = |G.out(v)|$ as its out-degree, and $C_0$ as $1 - \alpha/|V|$. Given the assumption that the rank contribution of its in-neighbors remains constant, the rank of $v$ after one iteration can be expressed as:

\begin{flalign*}
  r_1 & = \alpha (c + \frac{r_0}{d}) + C_0 && \\
      & = \alpha c + \alpha \frac{r_0}{d} + C_0 && \\
\end{flalign*}

\noindent
After the second iteration, the rank of the vertex would be:

\begin{flalign*}
  r_2 & = \alpha (c + \frac{r_1}{d}) + C_0 && \\
      & = \alpha (c + \frac{1}{d} (\alpha c + \alpha \frac{r_0}{d} + C_0)) + C_0 && \\
      & = \alpha c + \alpha^2 \frac{c}{d} + \alpha^2 \frac{r_0}{d^2} + \alpha \frac{C_0}{d} + C_0 &&
\end{flalign*}

\noindent
Following the third iteration, the vertex's rank would be:

\begin{flalign*}
  r_3 & = \alpha (c + \frac{r_2}{d}) + C_0 && \\
      & = \alpha (c + \frac{1}{d} (\alpha c + \alpha^2 \frac{c}{d} + \alpha^2 \frac{r_0}{d^2} + \alpha \frac{C_0}{d} + C_0) + C_0 && \\
      & = \alpha c + \alpha^2 \frac{c}{d} + \alpha^3 \frac{c}{d^2} + \alpha^3 \frac{r_0}{d^3} + \alpha^2 \frac{C_0}{d^2} + \alpha \frac{C_0}{d} + C_0 && \\
\end{flalign*}

\noindent
Expanding this to an infinite number of iterations, the vertex's final rank would be:

\begin{flalign*}
  r_\infty & = \frac{\alpha c}{1 - \alpha / d} + \frac{C_0}{1 - \alpha / d} && \\
           & = \frac{1}{1 - \alpha / d} (\alpha c + C_0)
\end{flalign*}

\noindent
Hence, the closed-loop formula for calculating the rank of a vertex $v$ in DF-P PageRank is:

\begin{flalign}
  R[v] & = \frac{1}{1 - \alpha / |G.out(v)|} \left(\alpha K + \frac{1 - \alpha}{|V|}\right) && \\
    \text{where, } K & = \left(\sum_{u \in G.in(v)} \frac{R[u]}{|G.out(u)|}\right) - \frac{R[v]}{|G.out(v)|}
\end{flalign}

\input{src/tab-dataset-large}
\input{src/fig-temporal-compare}
\input{src/fig-8020-runtime-compare}
\input{src/fig-8020-runtime-compare}




\subsection{Paritioning}
\label{sec:partition}

\subsection{Indirect Comparison with State-of-the-art PageRank Implementations (Static)}
\label{sec:static-comparison-indirect}

We now indirectly compare the performance of our GPU implementation of Static PageRank with other similar state-of-the-art implementations. Chen et al. \cite{chen2022atos} present Atos, a state-of-the-art task-parallel GPU scheduler for graph analytics. They say that in Gunrock and other frameworks, each frontier in a graph sweep is launched as a separate GPU kernel in the BSP model. This may result in insufficient parallelism, uneven finish times, and high kernel launch overhead for small frontiers. In contrast, Chen et al. present a persistent task scheduler which runs continuously to minimize kernel launch overhead, and also support asynchronous execution. For PageRank, they present a push-based asynchronous PageRank (requires many atomic ops) that uses a frontier to keep track of vertices that need to be processed in the next iteration. They use a queue to keep track fo the frontier (also requires atomic ops). Their CUDA kernel appears to be barrier-free. Their frontier concept is similar to ours \cite{sahu2024df}, but they do not use it for Dynamic PageRank. Chen et al. are able to achieve $3.2\times$ speedup over Gunrock on the \textit{indochina-2004} graph (see Table $1$ of their paper \cite{chen2022atos}). However, on the same graph, we achieve $24.4\times$ speedup over Gunrock (see Figure \ref{fig:compare--speedup} in this report).

In another work, Chen et al. \cite{chen2022scalable} extend their Atos dynamic scheduling framework to multi-node GPU systems that supports Partitioned Global Address Space (PGAS) style lightweight one-sided memory operations within and between nodes. However on the \textit{indochina-2004} graph, even with $4$ GPUs, they are unable to beat our speedup with respect to Gunrock (see Table $4$ of their paper \cite{chen2022scalable}, and Figure \ref{fig:compare--speedup} in this report).

Yang et al. \cite{yang2022graphblast} present GraphBLAST, A High-Performance Linear Algebra-based Graph Framework on the GPU. They discuss that GraphBLAS has lacked high-performance implementations for GPUs. Further, they say that GraphBLAS Template Library (GBTL), a GraphBLAS-inspired GPU graph framework, is an order of magnitude slower than state-of-the-art graph frameworks on the GPU in terms of performance. They say, the issue lies with the lack of generalizability of optimizations, irregular memory access patterns and load imbalance, and low compute-to-memory access ratio. Their new design principles include exploiting input sparsity, which allows users to write graph algorithms without specifying push and pull direction, exploiting output sparsity allows users to tell the backend which values of the output in a single vectorized computation they do not want computed, and load-balancing. For SpMV load balancing (like PageRank) they discuss two main approaches, row split, which seems like block-per-vertex; and merge-based, which seems like edge balanced between threads/blocks. For PageRank, they use merge-based load balancing with segmented scan. They also use a heuristic to switch between push- and pull-based approach. They say that the optimal time to switch from push to pull is very early on (as Ligra). On the \textit{indochina-2004} graph, they are able to achieve $2.2\times$/$1.2\times$ speedup over Gunrock (see Table $12$/$13$ in their paper \cite{yang2022graphblast}). However, on the same graph, we achieve $24.4\times$ speedup over Gunrock (see Figure \ref{fig:compare--speedup} in this report).

Wang et al. \cite{wang2021grus} present Grus, a Unified-memory-efficient High-performance Graph Processing on GPU. They focus on addressing the following, related to Unified Memory (UM): minimizing the amount of migrated data; reducing the number of page faults; and reducing page migration overhead. They achieve this with their framework with memory management and execution optimization. They use CSR as a space-efficient data structure for graph representation, and use $5|V|$ bytes for representing a frontier. They use an adaptive UM policy, where the frontier and ranks are assigned high priority, the CSR index array is assigned medium priority, and the CSR edges array is assigned low priority. They also use a Bitmap-directed frontier (8-bit integer array, similar to ours, plus a queue - no atomic ops needed), and use warp-centric load balancing (warp-per-vertex, similar to block-per-vertex, no partitioning) for PageRank computation. On the \textit{uk-2005} graph, they are able to achieve a $1.2\times$ speedup over Gunrock (see Table $4$ in their paper \cite{wang2021grus}). However, we get $8.6\times$ speedup over Gunrock on the same graph (see Figure \ref{fig:compare--speedup} in this report).

Concessao et al. \cite{concessao2023meerkat} propose a library-based framework for dynamic graph algorithms that utilizes a GPU-tailored graph representation and exploits the warp-cooperative execution model. The library, named Meerkat, builds upon a recently proposed dynamic graph representation on GPUs. This representation exploits a hashtable-based mechanism to store a vertexâ€™s neighborhood. Meerkat also enables fast iteration through a group of vertices, such as the whole set of vertices or the neighbors of a vertex. They find that these two iteration patterns are common, and optimizing them is crucial for achieving performance. Meerkat supports dynamic edge additions and edge deletions, along with their batched versions. The PageRank implementation of Meerkat performs, on average, $1.7\times$ faster than Hornet. However, our Static PageRank is on average $31\times$ faster than Hornet (see Figure \ref{fig:compare--speedup} in our report).
