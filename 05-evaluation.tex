\subsection{Experimental Setup}
\label{sec:setup}

\subsubsection{System used}

Experiments are performed on a system featuring an AMD EPYC-7742 processor with $64$ cores, operating at a frequency of $2.25$ GHz. Each core is equipped with a $4$ MB L1 cache, a $32$ MB L2 cache, and shares a $256$ MB L3 cache. The server is set up with $512$ GB of DDR4 system memory and runs Ubuntu $20.04$.


\subsubsection{Configuration}

We use 32-bit integers for vertex IDs and 64-bit floating-point numbers for vertex ranks. Affected vertices are represented with an 8-bit integer vector. Rank computation employs OpenMP's \textit{dynamic schedule} with a chunk size of $2048$ for dynamic workload balancing among threads. We set the damping factor to $\alpha = 0.85$ \cite{rank-langville06} and an iteration tolerance of $\tau = 10^{-10}$ using the $L_\infty$-norm \cite{rank-dubey22, rank-plimpton11}. The maximum number of iterations $MAX\_ITERATIONS$ is limited to $500$ \cite{nvgraph}. All experiments run with $64$ threads to match the available system cores, unless stated otherwise. Compilation is done using GCC $9.4$ and OpenMP $5.0$.


\subsubsection{Dataset}
\label{sec:dataset}

We utilize five temporal networks from the Stanford Large Network Dataset Collection \cite{snapnets}, outlined in Table \ref{tab:dataset}. These networks contain vertex counts ranging from $24.8$ thousand to $2.60$ million, temporal edge counts from $507$ thousand to $63.4$ million, and static edge counts from $240$ thousand to $36.2$ million. To address dead ends (vertices lacking out-links), a global teleport rank computation is needed in each iteration. We mitigate this overhead by adding self-loops to all vertices\ignore{ in the graph} \cite{kolda2009generalized, rank-andersen07, rank-langville06}.

\input{src/tab-dataset}


\subsubsection{Batch Generation}
\label{sec:batch-generation}

In each experiment, we initially load $90\%$ of every real-world dynamic graph from Table \ref{tab:dataset}, followed by loading $B$ edges consecutively in $100$ batch updates. Here, $B$ represents the desired batch size, specified as a fraction of the total number of temporal edges $|E_T|$ in the graph. Additionally, self-loops are added to all vertices with each batch update.


\subsubsection{Measurement}
\label{sec:measurement}

We evaluate the runtime of each approach on the entire updated graph, including preprocessing and convergence detection time, but excluding memory allocation/deallocation time. The mean time and error for a specific method at a given batch size is computed as the geometric mean across input graphs.\ignore{Average speedup is the ratio of these mean times.} Additionally, we assess the error/accuracy of each approach by measuring the $L1$-norm \cite{ohsaka2015efficient} of the ranks compared to ranks obtained from a reference Static PageRank run on the updated graph with an extremely low iteration tolerance of $\tau = 10^{-100}$ (limited to $500$ iterations).




\subsection{Performance comparison}

\subsubsection{Results on large graphs (Static)}

We now evaluate the performance of our GPU implementation of Static PageRank, and compare it with the performance of Static PageRank in the Hornet \cite{busato2018hornet} and Gunrock \cite{wang2016gunrock} graph processing frameworks on large (static) graphs from Table \ref{tab:dataset-large}. For Hornet, we use a CUDA C++ program to read each input graph with \texttt{GraphStd::read()}, perform \texttt{HornetInit}, create a \texttt{HornetGraph}, and set up \texttt{StaticPageRank} with a damping factor $\alpha$ of $0.85$, and iteration tolerance $\tau$ of $10^{-10}$, and limit the maximum number of iterations to $500$. We also define a new PageRank operator called \texttt{Max}, to compute $L_\infty$-norm of the absolute difference between the previous and current rank vectors (since we use $L_\infty$-norm for convergence detection), and use this for convergence detection (with \texttt{forAllnumV()}) instead of $L1$-norm (used by default). To perform the PageRank computation, we then use \texttt{StaticPageRank::run()}, and measure its runtime with \texttt{Timer<DEVICE>}. For Gunrock, we use a CUDA C++ program to read each input graph in Table \ref{tab:dataset-large} with \texttt{io::matrix\_market\_t::load()}, convert it to a Compressed Sparse Row (CSR) representation with \texttt{format::csr\_t::from\_coo()}, and build a graph with \texttt{graph::bui} \texttt{ld::from\_csr()}. We then perform PageRank computation with \texttt{gunrock::pr::run()} upon the loaded graph with a damping factor $\alpha$ of $0.85$, an iteration tolerance of $10^{-10}$, and limit the number of iterations to $500$ by modifying the \texttt{gunrock::pr::enactor\_t::is\_} \texttt{converged()} function (note that Gunrock uses $L_\infty$-norm for convergence detection by default), and record the runtime reported by it. Neither Hornet or Gunrock offer dynamic PageRank algorithms.

Figure \ref{fig:compare--runtime} illustrates the runtime of Hornet, Gunrock, and our Static PageRank on the GPU, for each graph in the dataset. On the \textit{sk-2005} graph, our Static PageRank computes the ranks of vertices with an iteration tolerance $\tau$ of $10^{-10}$ in $4.2$ seconds, achieving a processing rate of $471$ million edges/s. Figure \ref{fig:compare--speedup} shows the speedup of Our Static PageRank with respect to Hornet and Gunrock. Our Static PageRank is on average $31\times$ faster than Hornet, and $5.9\times$ times faster than Gunrock. This speedup is particularly high on the \textit{webbase-2001} graph and road networks with Hornet, and on the \textit{indochina-2004} graph with Gunrock. Further, our GPU implementation of Static PageRank is on average $24\times$ times faster than our parallel multicore implementation of Static PageRank.

\input{src/fig-compare}


\subsubsection{Results on large graphs with random updates}

We also evaluate the performance of our improved Dynamic Frontier (DF) and Dynamic Frontier with Pruning (DF-P) PageRank algorithms alongside Static, Naive-dynamic (ND), and Dynamic Traversal (DT) PageRank on large (static) graphs from Table \ref{tab:dataset-large}, with randomly generated batch updates. This is done on batch updates of size $10^{-7}|E|$ to $0.1|E|$ (in multiples of $10$), comprising $80\%$ edge insertions and $20\%$ edge deletions in order to simulate realistic batch updates. Edge insertions are generated by selecting vertex pairs with equal probability, while edge deletions involve deleting each existing edge with a uniform probability. No new vertices are added to or removed from the graph, and self-loops are added to all vertices with each batch update. Figure \ref{fig:8020-runtime} illustrates the runtime of Static, ND, DT, DF, and DF-P PageRank, while Figure \ref{fig:8020-error} depicts the error in ranks obtained with each approach.

\input{src/fig-8020-runtime}
\input{src/fig-8020-error}

Figure \ref{fig:8020-runtime--mean} illustrates that for batch updates ranging from $10^{-7}|E|$ to $10^{-4}|E|$, comprising $80\%$ insertions and $20\%$ deletions, DF PageRank is, on average, $2.5\times$, $1.3\times$, and $10.4\times$ faster than Static, ND, and DT PageRank respectively. Additionally, DF-P PageRank is, on average, $3.1\times$, $1.7\times$, and $13.1\times$ faster than Static, ND, and DT PageRank respectively. This speedup is particularly higher on road networks and protein k-mer graphs, which have a low average degree (as depicted in Figure \ref{fig:8020-runtime--all}). It's worth noting that DT PageRank is slower than ND PageRank \cite{sahu2024incrementally} on large (static) graphs with uniformly random batch updates, as it ends up marking a large number of vertices as affected. This is due to updates being randomly scattered across the graph, leading to most of the graph being reachable from the updated regions. Figures \ref{fig:8020-error--mean} and \ref{fig:8020-error--all} indicate that DF-P PageRank generally exhibits higher error compared to ND, DT, and DF PageRank, but lower error than Static PageRank (up to a batch size of $10^{-4}|E|$).\ignore{Consequently, DF PageRank is recommended as the preferred approach for large random batch updates.}

NOTE ABOUT COMPARE WITH CPU.

\subsubsection{Results on real-world dynamic graphs}

We now compare the performance of our Dynamic Frontier (DF) and Dynamic Frontier with Pruning (DF-P) PageRank algorithms with Static, Naive-dynamic (ND), and Dynamic Traversal (DT) PageRank on real-world dynamic graphs from Table \ref{tab:dataset}. This is done on batch updates of size $10^{-5}|E_T|$ to $10^{-3}|E_T|$ in multiples of $10$. For each batch size, we load $90\%$ of the graph initially and then load $B$ edges (where $B$ is the batch size) consecutively in $100$ batch updates. Self-loops are added to all vertices with each batch update. Figure \ref{fig:temporal-summary--runtime-overall} displays the overall runtime of each approach across all graphs for each batch size, while Figure \ref{fig:temporal-summary--error-overall} illustrates the overall rank error compared to a reference Static PageRank run (as described in Section \ref{sec:measurement}). Additionally, Figures \ref{fig:temporal-summary--runtime-graph} and \ref{fig:temporal-summary--error-graph} present the mean runtime and rank error of the approaches on each dynamic graph in the dataset. Finally, Figures \ref{fig:temporal-sx-mathoverflow}, \ref{fig:temporal-sx-askubuntu}, \ref{fig:temporal-sx-superuser}, \ref{fig:temporal-wiki-talk-temporal}, and \ref{fig:temporal-sx-stackoverflow} show the runtime and rank error of the approaches on each dynamic graph in Table \ref{tab:dataset}, upon each consecutive batch update.

Figure \ref{fig:temporal-summary--runtime-overall} shows that DF PageRank is, on average, $1.4\times$ faster than Static PageRank for batch updates of size $10^{-5}|E_T|$. In contrast, DF-P PageRank is, on average, $3.6\times$, $2.0\times$, and $1.3\times$ faster than Static PageRank for batch updates of size $10^{-5}|E_T|$, $10^{-4}|E_T|$, and $10^{-3}|E_T|$ respectively. Furthermore, DF-P PageRank is, on average, $4.2\times$, $2.8\times$, and $3.6\times$ faster than DT PageRank on identical batch updates. This speedup is particularly higher on the \textit{sx-mathoverflow} and the \textit{sx-askubuntu} dynamic graphs, with DF-P PageRank, as indicated by Figure \ref{fig:temporal-summary--runtime-graph}.

Regarding rank error, Figures \ref{fig:temporal-summary--error-overall} and \ref{fig:temporal-summary--error-graph} indicate that DF and DF-P PageRank have, on average, higher error than ND and DT PageRank but lower error than Static PageRank. This makes the ranks obtained with DF and DF-P PageRank acceptable. Therefore, DF-P PageRank can be the default choice for updating PageRank scores on dynamic graphs, but if higher error is observed (through intermediate empirical tests), switching to ND PageRank is recommended.

\input{src/fig-temporal-summary}


\subsubsection{Comparison of vertices marked as affected}

Figure \ref{fig:measure-affected} displays the (mean) percentage of vertices marked as affected by Dynamic Traversal (DT), our improved Dynamic Frontier (DF), and Dynamic Frontier with Pruning (DF-P) PageRank on real-world dynamic graphs from Table \ref{tab:dataset}. This analysis is conducted on batch updates of size $10^{-5}|E_T|$ to $10^{-3}|E_T|$ in multiples of $10$ (see Section \ref{sec:batch-generation} for details). For DF and DF-P PageRank, affected vertices are marked incrementally --- therefore, we count all vertices that were ever flagged as affected.

As Figure \ref{fig:measure-affected} indicates, the proportion of vertices marked as affected by DF and DF-P PageRank is lower than DT PageRank for batch updates of size $10^{-5}|E_T|$, but comparable for larger batch updates. Therefore, the performance improvement with DF and DF-P PageRank is primarily attributed to the incremental marking of affected vertices. Additionally, it's worth noting that the percentage of vertices marked as affected is generally low across all approaches. This is likely because updates in real-world dynamic graphs tend to be concentrated in specific regions of the graph rather than being scattered throughout.

\input{src/fig-measure-affected}
